{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPlaThLu7FD5XYHGMqJIUYr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### Linear & Logistic Regression\n",
        "\n",
        "- Logistic Regression에서 sigmoid를 쓰는 이유\n",
        "  - 0 or 1을 구분하기 위한 Activation Function이 sigmoid 이기 때문\n",
        "\n",
        "### Train/Test/Validation data\n",
        "\n",
        "  - Train 세트 : 훈련을 위해 사용되는 데이터\n",
        "  - Test 세트 : 모델이 얼마나 잘 작동하는지 확인\n",
        "  - Validation 세트 : 다양한 모델 중 어떤 모델이 좋은 성능 나타내는지 확인\n",
        "\n",
        "### Overfitting\n",
        "\n",
        "- 측정에 의한 데이터는 노이즈가 있을 수 있고 그 노이즈까지 학습해버리는 것을 과적합이라고한다.\n",
        "\n",
        "- 적당한 loss에서 끊어 줄 필요가 있고, validation set이 필요한 이유\n",
        "\n",
        "- validation loss에서 갑자기 상승하는 구간을 통해 적당한 epoch를 설정가능\n",
        "\n",
        "- 참고 \n",
        "  - https://untitledtblog.tistory.com/m/68 -> 과적합\n",
        "  - https://ko.wikipedia.org/wiki/%EA%B3%BC%EC%A0%81%ED%95%A9 -> 로지스틱에서 과적합\n",
        "\n",
        "\n",
        "### Fully Connected Layer\n",
        "\n",
        "  - 모든 뉴런이 연결된 레이어\n",
        "    - 항상 모든 뉴런이 연결될 필요가 없음, 뉴런이 모두 연결될 때 w값이 너무 많아져서 리소스가 불필요하게 늘어남\n",
        "\n",
        "### Activation Function\n",
        "\n",
        " - 비선형성을 위해서 Activation Function이 필요\n",
        "\n",
        " - softmax\n",
        "  - 이분법이 아니라 여러가지 output을 가질 수 있을 때, 확률로 만들어주는 activation function\n",
        "\n",
        "### Normalization/Standardization/Regularization\n",
        "\n",
        "- Normalization (정규화)\n",
        "  - Data를 0과 1사이의 값으로 재조정\n",
        "  - Input 간의 값 차이가 크면 학습이 잘 안됨\n",
        "\t  - 데이터크기가 100배 크면 Error도 100배 큼\n",
        "\n",
        "- Standardization (표준화)\n",
        "\n",
        "  - 평균을 0으로 하고 표준편차를 1로 만듭니다\n",
        "  - 참고\n",
        "    - https://m.blog.naver.com/mrp/221672080759\n",
        "\n",
        "- Regularization (정칙화)\n",
        "\n",
        "  - overfitting을 막아주는 방법\n",
        "  - 참고\n",
        "    - https://m.blog.naver.com/laonple/220527647084\n",
        "\n"
      ],
      "metadata": {
        "id": "Zd_qptNe6evp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUDxH2hfqa2M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad0e98ca-9b55-4844-f299-caaf0ce2066a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n",
            "[<tf.Variable 'dense_9/kernel:0' shape=(2, 4) dtype=float32, numpy=\n",
            "array([[ 2.4334505 ,  2.821953  , -0.3167658 ,  2.3563223 ],\n",
            "       [-2.4430728 ,  1.5909126 , -0.33014655,  2.6804435 ]],\n",
            "      dtype=float32)>, <tf.Variable 'dense_9/bias:0' shape=(4,) dtype=float32, numpy=array([-0.03113105, -1.2180376 ,  0.        , -0.604933  ], dtype=float32)>, <tf.Variable 'dense_10/kernel:0' shape=(4, 2) dtype=float32, numpy=\n",
            "array([[ 2.2533977 ,  2.8736522 ],\n",
            "       [-2.1630995 ,  3.5727046 ],\n",
            "       [-0.50213766,  0.50738454],\n",
            "       [-0.6580256 ,  3.4105844 ]], dtype=float32)>, <tf.Variable 'dense_10/bias:0' shape=(2,) dtype=float32, numpy=array([ 4.9933333, -3.347811 ], dtype=float32)>, <tf.Variable 'dense_11/kernel:0' shape=(2, 3) dtype=float32, numpy=\n",
            "array([[ 4.117405 ,  2.3446457, -6.8036594],\n",
            "       [-5.7943754,  3.1819534,  4.1658   ]], dtype=float32)>, <tf.Variable 'dense_11/bias:0' shape=(3,) dtype=float32, numpy=array([ 0.91015154, -1.4816067 ,  1.3711096 ], dtype=float32)>]\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "0,0 ->  0\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "0,1 ->  1\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1,0 ->  1\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1,1 ->  2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def onehot_to_class(prediction):\n",
        "  idx = prediction.argmax(axis = 0)\n",
        "\n",
        "  if idx == 0:\n",
        "    return str(0)\n",
        "  if idx == 1:\n",
        "    return str(1)\n",
        "  if idx == 2:\n",
        "    return str(2)\n",
        "  else :\n",
        "    return 'error'\n",
        "\n",
        "\n",
        "\n",
        "# input\n",
        "X_train = [[0,0],[0,1],[1,0],[1,1]]\n",
        "X_train = np.array(X_train)\n",
        "\n",
        "# output\n",
        "Y_train=[[0],[1],[1],[2]] # 숫자로 분류할 때\n",
        "# Y_train=[[1,0,0],[0,1,0],[0,1,0],[0,0,1]]\n",
        "\n",
        "Y_train = (tensorflow.keras.utils.to_categorical(Y_train))\n",
        "\n",
        "print(Y_train)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(units=4, activation='relu', input_shape=[2]),\n",
        "    keras.layers.Dense(units=2, activation='sigmoid'), \n",
        "    keras.layers.Dense(units=3, activation='softmax'), # 출력이 여러가지\n",
        "])\n",
        "\n",
        "# units = 해당 layer의 Neuron의 갯수\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=0.1)\n",
        "\n",
        "# model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy']) # 숫자로 분류할 때\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) # one-hot encoding\n",
        "\n",
        "hist = model.fit(X_train, Y_train, batch_size=4, epochs=100, shuffle=True, verbose = 0)\n",
        "\n",
        "print(model.weights)\n",
        "print(\"0,0 -> \", onehot_to_class(model.predict([[0,0]])[0]))\n",
        "print(\"0,1 -> \", onehot_to_class(model.predict([[0,1]])[0]))\n",
        "print(\"1,0 -> \",onehot_to_class(model.predict([[1,0]])[0]))\n",
        "print(\"1,1 -> \", onehot_to_class(model.predict([[1,1]])[0]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_list = np.array([0 , 10, 3, 5, 7])\n",
        "print(temp_list.argmax(axis=0)) # max 값이 들어있는 index 출력\n",
        "print(temp_list.argmin(axis=0)) # min 값이 들어있는 index 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tpstBRFznwA",
        "outputId": "2e81ec91-3bd7-42c1-9a8a-7c92c8ff1730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, Y_train), (X_test,Y_test) = keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "id": "3wQQki0vz3QW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}