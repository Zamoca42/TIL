{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMEtpm+1VORUxpuW9GE8ifc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zamoca42/TIL/blob/main/AI/keras_softmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### Linear & Logistic Regression\n",
        "\n",
        "- Logistic Regression에서 sigmoid를 쓰는 이유\n",
        "  - 0 or 1을 구분하기 위한 Activation Function이 sigmoid 이기 때문\n",
        "\n",
        "### Train/Test/Validation data\n",
        "\n",
        "  - Train 세트 : 훈련을 위해 사용되는 데이터\n",
        "  - Test 세트 : 모델이 얼마나 잘 작동하는지 확인\n",
        "  - Validation 세트 : 다양한 모델 중 어떤 모델이 좋은 성능 나타내는지 확인\n",
        "\n",
        "### Overfitting\n",
        "\n",
        "- 측정에 의한 데이터는 노이즈가 있을 수 있고 그 노이즈까지 학습해버리는 것을 과적합이라고한다.\n",
        "\n",
        "- 적당한 loss에서 끊어 줄 필요가 있고, validation set이 필요한 이유\n",
        "\n",
        "- validation loss에서 갑자기 상승하는 구간을 통해 적당한 epoch를 설정가능\n",
        "\n",
        "- 참고 \n",
        "  - https://untitledtblog.tistory.com/m/68 -> 과적합\n",
        "  - https://ko.wikipedia.org/wiki/%EA%B3%BC%EC%A0%81%ED%95%A9 -> 로지스틱에서 과적합\n",
        "\n",
        "\n",
        "### Fully Connected Layer\n",
        "\n",
        "  - 모든 뉴런이 연결된 레이어\n",
        "    - 항상 모든 뉴런이 연결될 필요가 없음, 뉴런이 모두 연결될 때 w값이 너무 많아져서 리소스가 불필요하게 늘어남\n",
        "\n",
        "### Activation Function\n",
        "\n",
        " - 비선형성을 위해서 Activation Function이 필요\n",
        "\n",
        " - softmax\n",
        "  - 이분법이 아니라 여러가지 output을 가질 수 있을 때, 확률로 만들어주는 activation function\n",
        "\n",
        "### Normalization/Standardization/Regularization\n",
        "\n",
        "- Normalization (정규화)\n",
        "  - Data를 0과 1사이의 값으로 재조정\n",
        "  - Input 간의 값 차이가 크면 학습이 잘 안됨\n",
        "\t  - 데이터크기가 100배 크면 Error도 100배 큼\n",
        "\n",
        "- Standardization (표준화)\n",
        "\n",
        "  - 평균을 0으로 하고 표준편차를 1로 만듭니다\n",
        "  - 참고\n",
        "    - https://m.blog.naver.com/mrp/221672080759\n",
        "\n",
        "- Regularization (정칙화)\n",
        "\n",
        "  - overfitting을 막아주는 방법\n",
        "  - 참고\n",
        "    - https://m.blog.naver.com/laonple/220527647084\n",
        "\n"
      ],
      "metadata": {
        "id": "Zd_qptNe6evp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUDxH2hfqa2M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "9d39f052-d711-4efa-efce-d3d2b8ecc1f6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f329bbc88ad5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Y_train=[[1,0,0],[0,1,0],[0,1,0],[0,0,1]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'to_categorial' is not defined"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# input\n",
        "X_train=[[0,0],[0,1],[1,0],[1,1]]\n",
        "\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# output\n",
        "Y_train=[[0],[1],[1],[2]] # 숫자로 분류할 때\n",
        "# Y_train=[[1,0,0],[0,1,0],[0,1,0],[0,0,1]]\n",
        "\n",
        "Y_train = to_categorial(Y_train)\n",
        "\n",
        "print(Y_train)\n",
        "'''\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(units=4, activation='relu', input_shape=[2]),\n",
        "    keras.layers.Dense(units=2, activation='sigmoid'), \n",
        "    keras.layers.Dense(units=3, activation='softmax'), # 출력이 여러가지\n",
        "])\n",
        "\n",
        "# units = 해당 layer의 Neuron의 갯수\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=0.1)\n",
        "\n",
        "# model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy']) # 숫자로 분류할 때\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) # one-hot encoding\n",
        "\n",
        "hist = model.fit(X_train, Y_train, batch_size=4, epochs=1000, shuffle=True, verbose = 1)\n",
        "\n",
        "print(model.weights)\n",
        "print(\"0,0 -> \",model.predict([[0,0]]))\n",
        "print(\"0,1 -> \",model.predict([[0,1]]))\n",
        "print(\"1,0 -> \",model.predict([[1,0]]))\n",
        "print(\"1,1 -> \",model.predict([[1,1]]))'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('Modelloss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(hist.history['accuracy'])\n",
        "plt.plot(hist.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o1h5JaqpE2zL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}