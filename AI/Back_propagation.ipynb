{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zamoca42/TIL/blob/main/Back_propagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr2mMnkMl1Pw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJTAUI0mnma1"
      },
      "outputs": [],
      "source": [
        "# 22.09.26 - feed forward\n",
        "# 22.09.28 - back propagation\n",
        "# 22.10.01 - back propagation for Multi-Data\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Neuron:\n",
        "  def __init__(self):\n",
        "      self.w = 0\n",
        "      self.b = 0\n",
        "\n",
        "  def __init__(self, w, b):\n",
        "    self.w = w\n",
        "    self.b = b\n",
        "\n",
        "  # feed forward\n",
        "\n",
        "  def activation(self, sigma):\n",
        "    return sigma\n",
        "\n",
        "  def feedForward(self, x):\n",
        "    sigma = self.w*x+self.b\n",
        "    return self.activation(sigma)\n",
        "\n",
        "  def feedForwardAndPrint(self, x):\n",
        "    print(\"x : \", x, \" y : \", self.feedForward(x))\n",
        "  \n",
        "  # Back Propagtion\n",
        "  \n",
        "  def ActGrad(self, x, y_want):\n",
        "    y = self.feedForward(x)\n",
        "    dEdW = (y-y_want)*1*1*x\n",
        "    dEdb = (y-y_want)*1*1*1\n",
        "    return dEdW, dEdb\n",
        "\n",
        "  def BackProp(self, x, y_want):\n",
        "    dEdW, dEdb = self.ActGrad(x,y_want)\n",
        "    alpha = 0.1\n",
        "    self.w = self.w - alpha*dEdW\n",
        "    self.b = self.b - alpha*dEdb\n",
        "  \n",
        "  def Cost(self,x,y_want):\n",
        "    y = self.feedForward(x)\n",
        "    cost = 0.5 * (y-y_want)*(y-y_want)\n",
        "    return cost\n",
        "\n",
        "nn = Neuron(1,3)\n",
        "\n",
        "# for i in np.arange(0,3,0.5):\n",
        "#   nn.feedForwardAndPrint(i)\n",
        "\n",
        "x = 2.5\n",
        "y_want = 2.5\n",
        "\n",
        "print(\"initial state\")\n",
        "print(\"w : \", nn.w, \"b : \",nn.b)\n",
        "print(\"y_want : \", y_want, \"y : \", nn.feedForward(x))\n",
        "\n",
        "for i in range(20):\n",
        "  print(\"i : \", i + 1)\n",
        "  nn.BackProp(x, y_want)\n",
        "  print(\"w : \", nn.w, \"b : \",nn.b)\n",
        "  print(\"y_want : \", y_want, \"y : \", nn.feedForward(x))\n",
        "  print(\"cost : \", nn.Cost(x,y_want))\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaqPfJJI09R5"
      },
      "outputs": [],
      "source": [
        "class BackPropagation(Neuron):\n",
        "  \n",
        "  def __init__(self,w,b):\n",
        "    super().__init__(w,b)\n",
        "  \n",
        "  def BackProp(self,x,y_want):\n",
        "    alpha = 0.1\n",
        "    # dEdW, dEdb = self.ActGrad(x,y_want)\n",
        "    y = self.feedForward(x)\n",
        "    dEdW = (y-y_want)*1*1*x\n",
        "    dEdb = (y-y_want)*1*1*1\n",
        "    self.w = self.w - alpha*dEdW\n",
        "    self.b = self.b - alpha*dEdb\n",
        "\n",
        "  def Cost(self,x,y_want):\n",
        "    y = self.feedForward(x)\n",
        "    cost = 0.5 * (y-y_want)*(y-y_want)\n",
        "    return cost\n",
        "\n",
        "Bp = BackPropagation(1,3)\n",
        "\n",
        "x = 2.5\n",
        "y_want = 2.5\n",
        "\n",
        "print(\"initial state\")\n",
        "print(\"w : \", Bp.w, \"b : \",Bp.b)\n",
        "print(\"y_want : \", y_want, \"y : \", Bp.feedForward(x))\n",
        "\n",
        "for i in range(20):\n",
        "  print(\"i : \", i + 1)\n",
        "  Bp.BackProp(x, y_want)\n",
        "  print(\"w : \", Bp.w, \"b : \",Bp.b)\n",
        "  print(\"y_want : \", y_want, \"y : \", Bp.feedForward(x))\n",
        "  print(\"cost : \", Bp.Cost(x,y_want))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gNK5BVF0qr9"
      },
      "outputs": [],
      "source": [
        "class BackPropagationMulti(Neuron):\n",
        "  def __init__(self,w,b):\n",
        "    super().__init__(w,b)\n",
        "\n",
        "  def assignData(self,X,Y):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "\n",
        "  def BackProp(self):\n",
        "\n",
        "    alpha = 0.1\n",
        "    dEdW_tot, dEdb_tot = 0,0\n",
        "\n",
        "    for i,j in zip(self.X, self.Y):\n",
        "      dEdW, dEdb = self.ActGrad(i,j)\n",
        "      dEdW_tot = dEdW_tot + dEdW\n",
        "      dEdb_tot = dEdb_tot + dEdb\n",
        "\n",
        "    self.w = self.w - alpha*dEdW_tot/len(self.X)\n",
        "    self.b = self.b - alpha*dEdb_tot/len(self.Y)\n",
        "    \n",
        "  def totalCost(self):\n",
        "    cost = 0\n",
        "    for i,j in zip(self.X, self.Y):\n",
        "      cost = cost + self.Cost(i,j)\n",
        "    return cost/len(self.X)\n",
        "\n",
        "Bp2 = BackPropagationMulti(1,3)\n",
        "\n",
        "x = [1,2,3]\n",
        "y_want = [2,4,6]\n",
        "\n",
        "Bp2.assignData(x,y_want)\n",
        "\n",
        "print(\"initial state\")\n",
        "print(\"w : \", Bp2.w, \"b : \",Bp2.b)\n",
        "print(\"x: 2.5 -> y: \", Bp2.feedForward(2.5))\n",
        "print(\"---\"*20)\n",
        "for i in range(1000):\n",
        "  print(\"i : \", i + 1)\n",
        "  Bp2.BackProp()\n",
        "  print(\"w : \", Bp2.w, \"b : \",Bp2.b)\n",
        "  print(\"x: 2.5 -> y: \", Bp2.feedForward(2.5))\n",
        "  print(\"cost : \", Bp2.totalCost())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB1eJ7FwrE4q"
      },
      "source": [
        "# 인공지능 학습\n",
        "\n",
        "  1. input\n",
        "  2. ouput\n",
        "  3. box -> hypothesis + 학습방법, cost, activation, ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5rfTgCzkWiG"
      },
      "outputs": [],
      "source": [
        "# keras를 이용한 방법\n",
        "\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([1,2,3])\n",
        "Y = np.array([1,2,4])\n",
        "\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(units=1, activation='linear', input_shape=[1]),\n",
        "    keras.layers.Dense(units=1, activation='linear'),\n",
        "    keras.layers.Dense(units=1, activation='linear'),\n",
        "])\n",
        "\n",
        "# cost function & 학습방법\n",
        "# mse = Mean(평균) Squared(제곱) Error(차이)\n",
        "# SGD -> Gradient Descent \n",
        "\n",
        "opt = keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "model.compile(loss='mse', optimizer=opt)\n",
        "\n",
        "model.fit(X,Y, batch_size=1, epochs=100)\n",
        "\n",
        "print(model.predict([2.5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ddTB_4KqBl1"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "X = np.random.rand(10)*20\n",
        "Y = np.zeros(10)\n",
        "X = np.append(X,np.random.rand(10)*20+80)\n",
        "Y = np.append(Y,np.ones(10))\n",
        "\n",
        "class LogisticReg(BackPropagationMulti):\n",
        "  def __init__(self,w,b):\n",
        "    super().__init__(w,b)\n",
        "  \n",
        "  # 1/(1+np.exp(-sigma)) - sigmoid\n",
        "\n",
        "  def activation(self, sigma):\n",
        "    return 1/(1+np.exp(-sigma))\n",
        "\n",
        "  def BackProp(self):\n",
        "    alpha = 0.01\n",
        "\n",
        "  def Cost(self, x, y_want):\n",
        "    y = self.feedForward(x)\n",
        "    cost = -y_want*np.log(y)-(1-y_want)*np.log(1-y)\n",
        "    return cost\n",
        "  \n",
        "  def ActGrad(self, x, y_want):\n",
        "    y = self.feedForward(x)\n",
        "    sigma = self.w*x+self.b\n",
        "    dfds = np.exp(-sigma)/((1+np.exp(-sigma))*(1+np.exp(-sigma)))\n",
        "\n",
        "    if y_want == 1 :\n",
        "      dEdW = -1/y*1*dfds*x\n",
        "      dEdb = -1/y*1*dfds*1\n",
        "    elif y_want == 0 :\n",
        "      dEdW = 1/(1-y)*1*dfds*x\n",
        "      dEdb = 1/(1-y)*1*dfds*1\n",
        "    \n",
        "    return dEdW, dEdb\n",
        "\n",
        "Bp2 = LogisticReg(1,3)\n",
        "\n",
        "\n",
        "Bp2.assignData(X,Y)\n",
        "\n",
        "print(\"initial state\")\n",
        "print(\"w : \", Bp2.w, \"b : \",Bp2.b)\n",
        "print(\"---\"*20)\n",
        "for i in range(1000):\n",
        "  print(\"i : \", i + 1)\n",
        "  Bp2.BackProp()\n",
        "  print(\"w : \", Bp2.w, \"b : \",Bp2.b)\n",
        "  print(\"x: 10 -> y: \", Bp2.feedForward(10))\n",
        "  print(\"x: 100 -> y: \", Bp2.feedForward(100))\n",
        "  print(\"cost : \", Bp2.totalCost())\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guJIRDYpyASH",
        "outputId": "ed698bd4-7bca-4f19-cfae-fc2fdaa7fd4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.06566107]]\n",
            "[[0.9898315]]\n"
          ]
        }
      ],
      "source": [
        "# keras를 이용한 Logistic Regression 방법\n",
        "\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "X = np.random.rand(20)*20\n",
        "Y = np.zeros(20)\n",
        "X = np.append(X,np.random.rand(20)*20+80)\n",
        "Y = np.append(Y,np.ones(20))\n",
        "\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(units=1, activation='sigmoid', input_shape=[1]),\n",
        "])\n",
        "\n",
        "# cost function & 학습방법\n",
        "# mse = Mean(평균) Squared(제곱) Error(차이)\n",
        "# SGD -> Gradient Descent \n",
        "\n",
        "opt = keras.optimizers.SGD(learning_rate=0.001)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "\n",
        "class CustomHist(keras.callbacks.Callback):\n",
        "  def init(self):\n",
        "    self.losses = []\n",
        "  \n",
        "  def on_epoch_end(self, batch, log={}):\n",
        "    self.losses.append(logs.get('loss'))\n",
        "\n",
        "custom_hist = CustomHist()\n",
        "custom_hist.init()\n",
        "\n",
        "hist = model.fit(X,Y, batch_size=1, epochs=1000, callbacks = 0,verbose = 0)\n",
        "\n",
        "# verbose 학습되는 epoch 보여줄지 여부\n",
        "# callback 학습 중에 취할 행동\n",
        "  \n",
        "\n",
        "print(model.predict([10]))\n",
        "print(model.predict([90]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StD-t4rS9IPJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hlhCCiCtG47"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOHY665530Q48Dg9HIfS/z3",
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
