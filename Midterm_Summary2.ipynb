{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsGCL8Rf9jOnG+pNTWjidX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### Linear & Logistic Regression\n",
        "\n",
        "- Logistic Regression에서 sigmoid를 쓰는 이유\n",
        "  - 0 or 1을 구분하기 위한 Activation Function이 sigmoid 이기 때문\n",
        "\n",
        "### Train/Test/Validation data\n",
        "\n",
        "  - Train 세트 : 훈련을 위해 사용되는 데이터\n",
        "  - Test 세트 : 모델이 얼마나 잘 작동하는지 확인\n",
        "  - Validation 세트 : 다양한 모델 중 어떤 모델이 좋은 성능 나타내는지 확인\n",
        "\n",
        "### Overfitting\n",
        "\n",
        "- 측정에 의한 데이터는 노이즈가 있을 수 있고 그 노이즈까지 학습해버리는 것을 과적합이라고한다.\n",
        "\n",
        "- 적당한 loss에서 끊어 줄 필요가 있고, validation set이 필요한 이유\n",
        "\n",
        "- validation loss에서 갑자기 상승하는 구간을 통해 적당한 epoch를 설정가능\n",
        "\n",
        "- 참고 \n",
        "  - https://untitledtblog.tistory.com/m/68 -> 과적합\n",
        "  - https://ko.wikipedia.org/wiki/%EA%B3%BC%EC%A0%81%ED%95%A9 -> 로지스틱에서 과적합\n",
        "\n",
        "\n",
        "### Fully Connected Layer\n",
        "\n",
        "  - 모든 뉴런이 연결된 레이어\n",
        "    - 항상 모든 뉴런이 연결될 필요가 없음, 뉴런이 모두 연결될 때 w값이 너무 많아져서 리소스가 불필요하게 늘어남\n",
        "\n",
        "### Activation Function\n",
        "\n",
        " - 비선형성을 위해서 Activation Function이 필요\n",
        "\n",
        " - Sigmoid\n",
        "  - $ a = \\frac{1}{1 + \\exp^{-z}} $\n",
        " - Tanh\n",
        "  - $ a = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} $\n",
        " - ReLU\n",
        "  - $ a = max(0,z) $\n",
        " - Leaky ReLU\n",
        "  - $ a = max(0.01z,z) $\n",
        " - softmax\n",
        "  - 이분법이 아니라 여러가지 output을 가질 수 있을 때, 확률로 만들어주는 activation function\n",
        "\n",
        "### Normalization/Standardization/Regularization\n",
        "\n",
        "- Normalization (정규화)\n",
        "  - Data를 0과 1사이의 값으로 재조정\n",
        "  - Input 간의 값 차이가 크면 학습이 잘 안됨\n",
        "\t  - 데이터크기가 100배 크면 Error도 100배 큼\n",
        "\n",
        "- Standardization (표준화)\n",
        "\n",
        "  - 평균을 0으로 하고 표준편차를 1로 만듭니다\n",
        "  - 참고\n",
        "    - https://m.blog.naver.com/mrp/221672080759\n",
        "\n",
        "- Regularization (정칙화)\n",
        "\n",
        "  - overfitting을 막아주는 방법\n",
        "  - 참고\n",
        "    - https://m.blog.naver.com/laonple/220527647084\n",
        "\n"
      ],
      "metadata": {
        "id": "Zd_qptNe6evp"
      }
    }
  ]
}